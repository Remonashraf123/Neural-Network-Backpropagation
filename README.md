# Neural Network Backpropagation

This project implements a simple neural network with one hidden layer using NumPy to demonstrate forward and backward propagation.

## Overview
The neural network consists of:
- **Input Layer**: 2 neurons
- **Hidden Layer**: 2 neurons
- **Output Layer**: 2 neurons
- **Activation Function**: Sigmoid
- **Learning Rate**: 0.5
- **Optimization Algorithm**: Gradient Descent
- **Loss Function**: Mean Squared Error

## Explanation
The project demonstrates the following steps:
- Forward propagation to calculate outputs
- Error calculation using mean squared error
- Backpropagation to adjust weights based on the error
- Weight updates using gradient descent
- Numerical demonstration of weight adjustments in each iteration

### Diagram
![image](https://github.com/user-attachments/assets/87f0a924-cd64-4160-b751-9a5f0fe67183)


## Prerequisites
- Python 3.x
- NumPy Library

## How to Run
1. Install Python and NumPy.
2. Clone the repository.
3. Navigate to the project directory.
4. Run the script using:
```bash
python neural_network.py
```

## Results
The final weights will be printed after one iteration of backpropagation, demonstrating the network's learning process.

## License
This project is for educational purposes only.

